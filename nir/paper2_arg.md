# Morphological Disambiguation for the Yakut Language

For interpretatibility: 007_02_0016 the imperative with beje


## Abstract


Morphological disambiguation entails selecting the correct morphological analysis for a word in context from multiple possible interpretations.
This paper addresses this task for Yakut (Sakha), a Turkic language spoken primarily in the Sakha Republic (Yakutia).
We employ an encoder-decoder recurrent neural network (RNN) with attention to rank candidate analyses generated by a rule-based morphological transducer, identifying the most contextually appropriate interpretation within a sentence.
Facing minimal morphologically annotated data, we experiment with data augmentation and subword tokenization strategies to mitigate data sparsity.
Our experiments demonstrate competitive disambiguation performance, advancing computational linguistics research on low-resource languages.


### Keywords
morphological disambiguation, Yakut language, encoder-decoder RNN, data augmentation, subword tokenization


## 1. Introduction

While subword tokenization algorithms have reduced the need for explicit morphological analysis in many natural language processing (NLP) tasks, such analysis remains critical for linguistic research and applications requiring fine-grained grammatical interpretation.

Morphological analysis maps a word’s surface form to its underlying morphological representation [1], producing decompositions grounded in linguistic theory.
This process is highly language-specific and particularly challenging for agglutinative languages like Yakut, where a single root can generate a vast number of word forms through sequential affixation [2].
This morphological richness leads to data sparsity, limiting the effectiveness of purely data-driven methods, such as neural networks, especially in low-resource languages like Yakut with limited annotated data.

Rule-based morphological transducers can effectively model the productive affixation processes of agglutinative languages using a finite set of linguistic rules.
However, the mapping between surface forms and morphological analyses is often ambiguous, with a single word form corresponding to multiple valid interpretations.
Morphological disambiguation—the task of selecting the correct analysis given the context—is thus a critical step in processing such languages [3].

This paper addresses morphological disambiguation for Yakut by evaluating a dual encoder with attention model, designed to score candidate analyses generated by a rule-based morphological transducer [4].
Unlike traditional encoder-decoder architectures, our approach employs two parallel encoders: one to encode the input sentence and another to encode candidate morphological analyses.
An attention mechanism allows the candidate representations to query the sentence context, producing a context-aware representation that is combined with the candidate encoding to compute relevance scores.
This architecture is well-suited for ranking candidate analyses by capturing contextual dependencies without requiring sequential generation.

In our approach:
- The sentence encoder generates context-aware embeddings for each token in the input sentence, incorporating rich linguistic features.
- The candidate encoder processes each candidate morphological analysis, producing a fixed-size representation.
An attention mechanism aligns candidate representations with relevant sentence contexts, followed by a scoring layer to rank candidates.

This hybrid approach combines the strengths of rule-based and data-driven methods.
The rule-based transducer ensures comprehensive coverage of morphologically valid forms, while the dual encoder with attention model leverages contextual information to resolve ambiguities efficiently.

To address data sparsity, we employ fine-grained subword tokenization and data augmentation techniques to expand the training data. Our aim is to maximize disambiguation accuracy while minimizing reliance on large amounts of annotated data. We train and evaluate our model on a dataset of sequences with manually disambiguated morphological parses.

The remainder of this paper is organized as follows: Section 2 reviews the dual encoder with attention architecture, drawing on related work in sentence-candidate scoring [4]. Section 3 describes the data used for training and testing. Section 4 details the model design and implementation. Section 5 discusses the types of morphological ambiguities addressed, with evaluation results presented in Section 6. Section 7 concludes the paper.


## 2. Previous Work

To the best of our knowledge, there is no dedicated work on morphological disambiguation for the Yakut language.
Hakkani-Tür et al. [5] provide a comprehensive survey of morphological disambiguation approaches for Turkish up to 2017.
These methods span early rule-based techniques—including both hand-crafted and machine-learned rules—to statistical and machine learning approaches such as hidden Markov models (HMMs), perceptron-based discriminative methods, and conditional random fields (CRFs).
To mitigate data sparsity in statistical models, strategies such as simplifying output representations by retaining only the primary part-of-speech [6], focusing on the final inflectional group, or disambiguating roots and affixes separately [7] have been proposed.

Most approaches reviewed by Hakkani-Tür et al. precede the advent of neural architectures suited for contextual sequence processing, such as the dual encoder with attention model used in our work.
Dual encoder architectures, which process two input sequences in parallel to produce comparable representations, have been widely applied to tasks involving ranking or scoring, such as sentence similarity and response selection.
For instance, Mueller and Thyagarajan [8] introduced a Siamese LSTM-based dual encoder for semantic textual similarity, encoding two text inputs separately and computing a similarity score, a paradigm adaptable to scoring candidate morphological analyses against a sentence context.

A key advancement relevant to our approach is the integration of attention mechanisms into dual encoder frameworks, as explored by Li et al. [9] for dialogue response ranking.
In their model, two encoders (often bidirectional RNNs like GRUs) separately encode a context and candidate responses, with an attention mechanism allowing the candidate representation to query the context’s hidden states.
This produces a context-aware representation that is combined with the candidate encoding for scoring, closely aligning with our architecture’s design for morphological disambiguation.
Similarly, Xing et al. [10] proposed a dual encoder model with attention for topic-aware response generation, using bidirectional GRUs to encode sentence and candidate inputs, followed by attention to align and score candidates based on contextual relevance.

These dual encoder with attention models are particularly suited for our task, as they efficiently handle the scoring of multiple candidate morphological analyses generated by a rule-based transducer, leveraging contextual dependencies without the need for sequential generation.
Unlike traditional encoder-decoder models used for sequence-to-sequence tasks (e.g., machine translation [11]), our approach focuses on ranking valid analyses, making the parallel encoding and attention-based alignment of dual encoders a natural fit.


## 3. Model Architecture

We employ a dual encoder with attention model to score a list of pre-generated morphological analyses produced by a rule-based transducer for a given Yakut sentence $ S = [t_1, t_2, \dots, t_n] $, where each $ t_i $ represents a subword token. The objective is to disambiguate the morphological analysis of a target word by scoring a set of candidate analyses $ {a_{i1}, a_{i2}, \dots, a_{im}} $ provided by the transducer.

### 3.1 Sentence Encoder

The sentence encoder is a bidirectional gated recurrent unit (GRU) network that generates contextual representations of the input sentence. To focus on the target word, we enclose its corresponding subword tokens $ t_i, t_{i+1}, \dots, t_{i+k} $ with special tokens $ \text{<TARGET>} $ and $ \text{</TARGET>} $. The modified input sequence is thus:

$ S' = [t_1, \dots, t_{i-1}, \text{<TARGET>}, t_i, \dots, t_{i+k}, \text{</TARGET>}, t_{i+k+1}, \dots, t_n] $

Each subword token $ t_j $ (including the special tokens) is represented as a learned embedding, initialized randomly and optimized during training to capture Yakut-specific morphological and phonological patterns. These embeddings are obtained via an embedding layer $ \text{word_embed}: \mathbb{R}^{|V| \to D} $, where $ |V| $ is the vocabulary size and $ D $ is the embedding dimension. The bidirectional GRU processes $ S' $, computing forward $ (\overrightarrow{h_j}) $ and backward $ (\overleftarrow{h_j}) $ hidden states for each token, which are concatenated to form the contextual representation $ h_j = [\overrightarrow{h_j}, \overleftarrow{h_j}] $. The sentence encoder outputs a sequence of hidden states:


$ H = [h_1, h_2, \dots, h_{n+2}] $,

$ H_s = [h_1, h_2, \dots, h_{n+2}] \in \mathbb{R}^{(n+2) \times 2H} $,

where $ n+2 $ accounts for the additional $ \text{<TARGET>} $ and $ \text{</TARGET>} $ tokens, and $ 2H $ is the hidden dimension of the bidirectional GRU.



### 3.2 Candidate Encoder

The candidate encoder, also a bidirectional GRU, processes each candidate morphological analysis $ a_{im} = [c_{m1}, c_{m2}, \dots, c_{ml}] $, where $ c_{mj} $ are subword tokens representing the morphological decomposition (e.g., root and affixes). Each candidate token is mapped to an embedding via a separate embedding layer $ \text{morph_embed}: \mathbb{R}^{|V_m| \to D} $, where $ |V_m| $ is the morphological vocabulary size. The bidirectional GRU computes forward and backward hidden states for each candidate, producing a sequence of contextual representations:

$ H_{c_m} = [h_{m1}, h_{m2}, \dots, h_{ml}] \in \mathbb{R}^{l \times 2H} $.

To obtain a fixed-size representation for each candidate, we apply mean pooling over the sequence, weighted by a candidate mask $ M_c \in {0,1}^{m \times l} $ to ignore padding tokens:

$ v_m = \frac{\sum_{j=1}^{l} h_{mj} \cdot M_c[m,j]}{\sum_{j=1}^{l} M_c[m,j]} \in \mathbb{R}^{2H} $,
where $ v_m $ is the pooled representation of candidate $ a_{im} $.

### 3.3 Attention and Scoring

To align each candidate with the sentence context, we employ an attention mechanism where the candidate representation $ v_m $ queries the sentence’s hidden states $ H_s $. The attention scores are computed using a linear projection:

$ e_{m,j} = v_m^\top W_a h_j $,

where $ W_a \in \mathbb{R}^{2H \times 2H} $ is a learned parameter matrix, and $ h_j \in H_s $ is the $ j $-th sentence hidden state. The scores are masked using the sentence mask $ M_s \in {0,1}^{n+2} $ to exclude padding tokens and normalized via softmax:

$ \alpha_{m,j} = \frac{\exp(e_{m,j}) \cdot M_s[j]}{\sum_{k=1}^{n+2} \exp(e_{m,k}) \cdot M_s[k]} $.

The context vector for each candidate is computed as a weighted sum:

$ c_m = \sum_{j=1}^{n+2} \alpha_{m,j} h_j \in \mathbb{R}^{2H} $.

The context vector $ c_m $ is concatenated with the candidate representation $ v_m $ to form a combined representation:

$ z_m = [c_m, v_m] \in \mathbb{R}^{4H} $.

This is passed through a feed-forward layer with a tanh activation to produce a hidden representation:

$ u_m = \tanh(W_c z_m + b_c) \in \mathbb{R}^{H} $,

where $ W_c \in \mathbb{R}^{4H \times H} $ and $ b_c \in \mathbb{R}^{H} $ are learned parameters. Finally, a scoring layer computes a scalar score for each candidate:

$ \text{score}_m = W_s u_m + b_s \in \mathbb{R} $,

where $ W_s \in \mathbb{R}^{H \times 1} $ and $ b_s \in \mathbb{R} $. The scores for all candidates $ { \text{score}_1, \text{score}_2, \dots, \text{score}_m } $ are passed through a softmax layer to obtain a probability distribution over the candidate analyses, enabling the model to select the most likely morphological analysis.

The model evaluates all candidates independently in parallel, leveraging the dual encoder structure to efficiently score multiple analyses without sequential processing.


## 4. Tokenizer and Model Training

The training set consists of sequences with morphological annotations that have been manually disambiguated.

Given the small size of the training data, we apply strategies to augment the training samples and use fine-grained subword tokenization to produce less sparse observations, reducing the risk of overfitting and enabling the model to learn generalizable patterns.

### 4.1 Data Augmentation

Incorporating special tokens to mark the target word provides an additional benefit of data augmentation. By sliding the target marker window along the sequence, the sentence encoder produces slightly different context embeddings for each target word, enhancing the model’s exposure to varied contexts.

Let $ D = {S_1, S_2, \dots, S_N} $ denote a corpus consisting of $ N $ morphologically annotated sequences. Each sequence $ S_i $ contains $ I_i $ input words, denoted as $ (i_{i1}, i_{i2}, \dots, i_{iI_i}) $, and $ T_i $ corresponding target analyses, denoted as $ (t_{i1}, t_{i2}, \dots, t_{iT_i}) $. For each word $ i_{ij} $, we generate a distinct training example where the target output is $ t_{ij} $. Therefore, the total number of training examples $ M $ generated from the corpus is:

$$ M = \sum^N_{i=1} I_i $$

Each sequence is augmented by the number of word forms it contains, significantly expanding the small size of our manually disambiguated corpus (12,000 sentences).

We also train the model on unambiguous analyses. While these cases do not directly contribute to disambiguating competing analyses, they allow the sentence encoder to learn robust, context-aware embeddings for a broader range of words and sentences. The context of unambiguous words (e.g., surrounding words, syntactic structure) enhances the encoder’s ability to model contextual dependencies, which is critical for resolving ambiguous cases. Additionally, unambiguous parses serve as “easy” positive examples, providing stable gradients during training and balancing the learning process when negative samples (incorrect analyses) introduce noise or complexity.

### 4.2 Subword Tokenization

We use the SentencePiece algorithm [1] to tokenize both input and candidate sequences, aiming to approximate morphemes or syllables for input sequences and segment lexical roots for candidate sequences.

For input sequences, the tokenizer segments words into subwords reflecting their morphological structure.
The ideal subword count for each word is estimated by analyzing its annotated morphological structure: the number of affixes (excluding the lexical root) plus the number of syllables in the root.
Syllable counts for roots are determined using a syllable segmentation algorithm.
If the algorithm fails (e.g., for roots deviating from standard Yakut orthography), the syllable count is approximated as the root’s character length divided by three, reflecting the average syllable length in Yakut.
For example:

- Input word: колхозтан
- Root: кол,хоз (2 syllables)
- Affixes: +N, +ABL (2 affixes, 1 counted after excluding the root marker)
- Total subwords: 3

The ideal subword count distribution for the input tokenizer is modeled as $ W = { len(w_1), len(w_2), \dots, len(w_n) } $, where $ w_i $ is a unique word in the corpus, and $ len(w_i) $ is its estimated subword count (syllables + affixes). We train the input tokenizer across a range of vocabulary sizes (±75% of the number of unique syllables in the corpus) and select the size that minimizes the Earth Mover’s Distance (EMD) [2] between the resulting subword count distribution and the ideal distribution.

For candidate sequences, the tokenizer segments only lexical roots, as affixes are represented as hardcoded special tokens.
The ideal subword count distribution is modeled as $ R = { len(r_1), len(r_2), \dots, len(r_n) } $, where $ r_i $ is a root in the morphological transducer’s lexicon, and $ len(r_i) $ is its syllable count, estimated as described above.
The candidate tokenizer is trained similarly, selecting the vocabulary size that minimizes EMD between the subword count distribution and the ideal distribution.

EMD is used to compare subword count distributions because it accounts for both the magnitude and ordinal distance of divergences, making it suitable for evaluating segmentation granularity.
This methodology ensures robust tokenization, enabling the dual encoder to capture morphological and semantic patterns effectively.
Consistent subword unit lengths in both input and candidate tokenizers also enhance the interpretability of the attention mechanism by aligning attention weights across sequences.


### 4.3 Training Hyperparameters

Hyperparameters, such as the number of GRU layers, hidden state size, and attention projection dimensions, are tuned via cross-validation on a held-out development set.

The model is trained to score candidate morphological analyses, treating the task as a classification problem.
For each input sentence and target word, the model receives a set of candidate analyses from the transducer, including the correct analysis. The model computes scores for all candidates, and a softmax layer converts these into a probability distribution.
We use cross-entropy loss to maximize the probability of the correct analysis, with the index of the true analysis within the candidate set as the target label.

The model was trained with the following hyperparameters:

- Embedding Size: 64 for subword and morphological token embeddings (word_embed and morph_embed), learned during training.
- Hidden State Size: 128 for sentence and candidate GRUs (sentence_encoder and candidate_encoder), balancing expressiveness and dataset size.
- Attention Projection: Linear layer (self.attn) mapping $ 2H \to 2H $ for attention score computation.
- Combination Layer: Linear layer (self.combine) mapping $ 4H \to H $ for combining context and candidate representations.
- Scoring Layer: Linear layer (self.scorer) mapping $ H \to 1 $ for candidate scores.
- Optimizer: Adam with a learning rate of 0.001.
- Dropout: 0.3 to prevent overfitting.
- Batch Size: 32, optimized for GPU training.

The training corpus consists of 12,000 manually disambiguated Yakut sentences, tokenized into subword units using a SentencePiece model trained on the corpus.
Each sentence was preprocessed to generate modified sequences with $ \text{<TARGET>} $ and $ \text{</TARGET>} $ tokens enclosing each word’s subword tokens, paired with a set of candidate analyses from the transducer, including the gold-standard analysis.
For each training example, the sentence is encoded once, and the candidate set (up to $ k $ analyses) is encoded in parallel, with scores computed via the attention and scoring layers.

Training was conducted for 15 epochs, with early stopping based on validation set performance.
The cross-entropy loss with softmax ensures the model learns to assign higher probabilities to correct analyses, effectively ranking them above incorrect ones.

## 5. Types of Morphological Ambiguity

Morphological ambiguity in Yakut arises from multiple sources.
Hakkani-Tür et al. [SOURCE] distinguish four key types for Turkish—part-of-speech ambiguity, segmentation ambiguity, morphographemic processes, and homograph morphemes—all of which are also observed in Yakut, each presenting distinct challenges for disambiguation.

Part-of-speech ambiguity often originates in the lexicon when a single lexical root is associated with multiple primary stem types. For example, the root *санаа* can be interpreted as a verbal stem (second person imperative of “to think”) or a nominal stem (singular nominative of “thought”).
Affixation can help resolve such lexical ambiguity, as stem types impose constraints on permissible affixes. Additionally, lexicalized derivations, where derived forms develop independent or idiomatic meanings, introduce further ambiguities [TUR].
Some monosyllabic adjectives coincide with verbs in both form and meaning, functioning as grammatical homonyms [§255, 161].

Segmentation ambiguity is prevalent due to the multiple ways a word form can be segmented, particularly in Yakut’s disyllabic verbal roots with “fossilized” productive affixes in the final syllable (see §500 on dead roots in reflexive verbs). For instance, the form *кэбис* can be analyzed as a single lexical root meaning “to throw, toss, pitch, cast” or as кэп (“push away, shove”) plus the reciprocal affix *-ис*. Segmentation ambiguity also arises with affix combinations, such as possessive cases relative to the plural. For example, the form *доруойдеталарыгар* can be segmented as:

доруобуйа+N+POSS.3PL+DAT
доруобуйа+N+PL+POSS.3SG+DAT

Here, the root *доруоба* (“health”) can be interpreted as singular or plural, requiring contextual disambiguation.

Morphographemic processes introduce ambiguity through phonological interactions. For example, the verbal roots *көп* (“to rise, float to the surface”) and *көт* (“to fly”) share the same past participle form *көппүт*. Due to regressive assimilation, the past participle affix *-быт* (starting with a labial consonant) causes the final dental voiceless stop *т* in *көт* to become a bilabial voiced stop *п*. As both roots are verbs, *көппүт* can only be disambiguated semantically based on context.
Another example is *барыах*, which can be analyzed as the verbal root *бар* (“to go, leave, depart”) with the future participle *-ыах*:

*бар*+V+PART_FUT

However, vowel-initial affixes like *-ыах* cause deletion of long consonants in stem endings [SOURCE], allowing an alternative interpretation as the verbal root *бараа* (“to spend”):

*бараа*+V+PART_FUT

Homograph morphemes further complicate disambiguation.
For instance, Yakut’s predicate and possessive affixes share identical forms for the first and second person plural (*-быт*, *-гыт*, and their allomorphs) [Ubryatova, 311]. Similarly, the affix *-нан* and its allomorphs can represent the instrumental case in both plain and possessive variants.
When homograph affixes have similar grammatical functions, syntactic cues alone are insufficient for disambiguation, necessitating deeper contextual analysis.

The dual encoder with attention model addresses these ambiguities by leveraging its architecture to capture contextual dependencies and score candidate analyses.
The sentence encoder processes the input sequence, producing context-aware embeddings that incorporate surrounding words and syntactic structure.
The candidate encoder independently processes each morphological analysis from the transducer, generating a fixed-size representation.
The attention mechanism allows candidate representations to query the sentence’s hidden states, focusing on contextually relevant tokens (e.g., those marked by $ \text{<TARGET>} $ and $ \text{</TARGET>} $) to resolve ambiguities like part-of-speech or segmentation differences.
By combining these representations and scoring them, the model learns statistical regularities and contextual patterns from the training data, effectively distinguishing between competing analyses for lexical, structural, phonological, and formal ambiguities.


## 6. Evaluation


The testing set contains 300 sentences compiled from a work on Yakut morpholgy [SOURCE], following the same methodology.

This is particularly effective for Yakut’s local syntactic dependencies, such as agreement in case or number, where surrounding syllables provide critical cues.




## 7. Conclusions and Further Work

We implemented a encoder-decoder model with attention to score candidate morphological analyses provided by the parser.

This hybrid approach leverages the parser’s comprehensive morphological coverage and the LSTM’s contextual sensitivity.

The architecture is designed to leverage limited annotated data while effectively capturing contextual dependencies critical for disambiguating affixes and lexical roots.


The rule-based morphological parser generates a set of possible analyses for each ambiguous word based on predefined linguistic rules and a lexicon. These analyses often include all morphologically valid forms, without considering contextual constraints. Our LSTM-based model complements the parser by leveraging contextual information to rank these analyses. For each ambiguous word in a sentence, the parser outputs a set of candidate analyses, which are then scored by the LSTM model. The highest-scoring analysis is selected as the disambiguated output.


Batch Candidates
When you batch the candidates, you are essentially passing multiple sequences through the encoder-decoder architecture in parallel, rather than one at a time. This approach takes advantage of parallelization (either on the CPU or GPU), reducing the overall time it takes to evaluate all candidates.

Conditional scoring for structured prediction


# References

[1] Sproat R. (1992). Morphology and Computation. ACL-MIT Press series in natural-language processing. 1992 Massachusetts Institute of Technology.

[2] Oflazer K., Saraçlar M. eds (2017) Turkish and Its Challenges for Language and Speech Processing. In: Turkish Natural Language Processing.

[3] Hakkani-Tür D., Oflazer K., Tür G. (2002). Statistical Morphological Disambiguation for Agglutinative Languages, 2002.

[4] Cortegoso Vissio N., Khokhlova M.A. (2024). Three-Stage Morphological Pipeline for Word Form Analysis in Yakut. Conference: 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE), pp. 1860-1864.

[5] Hakkani-Tür D.Z., Saraçlar M., Tür G., Oflazer K., Yuret D. (2017) Morphological disambiguation for Turkish. In: Turkish Natural Language Processing.

[6] Ehsani R., Alper M.E., Eryig ̆it G., Adalı E (2012) Disambiguating main POS tags for Turkish. In: Proceedings of the 24th conference on computational linguistics and speech processing, Chung-Li.

[7] Hakkani-Tür D.Z., Oflazer K., Tür G. (2002) Statistical morphological disambiguation for agglutinative languages. Comput Hum 36(4):381–410.

[8] Mueller J., Thyagarajan A. (2016) Siamese Recurrent Architectures for Learning Sentence Similarity.

[9] Li et al. (2020) Toward Interpretability of Dual-Encoder Models for Dialogue Response Suggestions.

[10] Xing et al. 

[11] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprint arXiv:1409.0473.





[11] Kann, K., & Schütze, H. (2016). MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection. In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology.

[12] Cotterell, R., & Heigold, G. (2017). Cross-lingual Character-level Neural Morphological Tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).

[13] Aharoni, R., & Goldberg, Y. (2017). Morphological Inflection Generation with Hard Monotonic Attention. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).







[2] Rubner, Y., Tomasi, C., & Guibas, L. J. (2000). The Earth Mover’s Distance as a metric for image retrieval. International Journal of Computer Vision, 40(2), 99–121.


[] Morphological disambiguation of Turkish text with perceptron algorithm.




[6] Hulden, M., & Francom, J. (2012). Boosting statistical tagger accuracy with simple rule-based grammars. LREC 2012.
[7] Silfverberg, M., Ruokolainen, T., Lindén, K., & Kurimo, M. (2016). FinnPos: An open-source morphological tagging and lemmatization toolkit for Finnish. Language Resources and Evaluation, 50(4), 863-878.
[8] Cotterell, R., Müller, T., Fraser, A., & Schütze, H. (2015). Labeled morphological segmentation with semi-Markov models. CoNLL 2015.
[9] Zalmout, N., & Habash, N. (2019). Adversarial multitask learning for joint multi-feature and multi-dialect morphological modeling. ACL 2019.
[10] Sheyanova, M., & Tyers, F. M. (2017). Annotation schemes in North Slavic corpora. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing.


## JUNK


Linguistic representation) tends to be structured, hierarchical, and highly dependent on linguistic theory, not just data frequency.



## 2. Previous Work



Sequence-to-sequence (seq2seq) learning was introduced by Sutskever et al. [[3]](#references), proposing an encoder-decoder architecture for mapping variable-length input sequences to variable-length output sequences.
Their model, based on recurrent neural networks, laid the foundation for many subsequent developments in machine translation and other structured prediction tasks.
The introduction of attention mechanisms by Bahdanau et al. [[4]](#references) further enhanced the seq2seq framework by allowing the decoder to selectively focus on relevant parts of the input sequence during generation, significantly improving performance on long and complex sequences.

Traditionally, encoder-decoder architectures have been used as generative models, producing outputs autoregressively based on previously generated tokens and the internal encoder representation.
However, several works have explored ways to adapt these models for discriminative tasks. Ranzato et al. [[5]](#references) proposed sequence-level training objectives for recurrent networks, showing that training models to rank entire candidate sequences can lead to better performance than token-level prediction. Similarly, Shen et al. [[6]](#references) introduced discriminative training strategies that enable models to directly optimize over candidate rankings rather than generation.
In information retrieval, MacAvaney et al. [[7]](#references) demonstrated that neural models with contextualized embeddings could be effectively trained to score and rank candidate outputs. Relatedly, Bahdanau et al. [[8]](#references) showed that attention-based models could be adapted for speech recognition tasks by discriminatively selecting among competing hypotheses during decoding.

While most work has focused on training models to generate or rank sequences in high-resource settings, relatively few efforts have investigated adapting encoder-decoder models for discriminative scoring in low-resource contexts, where only minimal annotated data is available.
Our work builds on these ideas by proposing a encoder-decoder architecture that scores candidate morphological analyses of a given word form, rather than generating them, allowing effective morphological disambiguation for Yakut despite the limited training data.



[3] Sutskever, I., Vinyals, O., & Le, Q.V. (2014). Sequence to Sequence Learning with Neural Networks. ArXiv.
[4] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate.
[5] Ranzato M.,  Chopra S., Auli M., Zaremba W. (2016). Sequence Level Training with Recurrent Neural Networks. International Conference on Learning Representations (ICLR).
[6] Shen Y., He X., Gao J., Deng L., Mesnil G. (2015). Training Neural Networks with Noisy Labels via Bootstrapping. International Conference on Learning Representations (ICLR) Workshop.
[7] MacAvaney S., Yates A., Cohan A., Goharian N. (2019). CEDR: Contextualized Embeddings for Document Ranking. Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval.
[8] Bahdanau D., Chorowski J., Serdyuk D., Brakel P., Bengio Y. (2016). End-to-End Attention-Based Large Vocabulary Speech Recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).






- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.




Morphological disambiguation is typically harder for agglutinative languages, such as Yakut for the following reasons:

- High Morpheme Density: A single word can encode multiple grammatical features, requiring disambiguation of overlapping or nested morpheme boundaries.

- Ambiguity in Segmentation: Morphemes may have homophonous forms (e.g., a suffix might mark a different morphological feature depending on context).

- Sparse Data: Rare morpheme combinations may not appear in training data, complicating statistical disambiguation.

The last point is particulary challenging for applying machine learning models (e.g., neural networks, CRFs) to the disambiguation task, as they rely on frequency statistics.
The training corpus may not contain enough examples of certain word forms or morpheme combinations to reliably estimate their probabilities.



The bootleneck is in the decoder part.




to select correct output among candidates in the context of the sequence.


- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.







A morphological transducer generates an analysis representation from a word's surface form, capturing its grammatical structure and lexical properties.


Morphological disambiguation is the task of selecting the correct sequence of morphological parses corresponding to a sequence of words, from the set of possible parses for those word forms [1].

This disambiguation is particularly challenging for agglutinative languages like Yakut (Sakha), where a single root can generate numerous distinct word forms through sequential affixation of grammatical morphemes.



PREVIOUS WORK

This hybrid approach of combining rule-based transducers with statistical or neural ranking has been explored in various contexts. Hulden and Francom [6] demonstrated the effectiveness of combining finite-state transducers with statistical methods for morphological disambiguation in Spanish. Similarly, Silfverberg et al. [7] employed neural networks to rerank the outputs of a morphological generator, showing significant improvements over pure rule-based systems.

For morphologically rich languages, Cotterell et al. [8] explored neural morphological disambiguation as a structured prediction task, while Zalmout and Habash [9] developed a neural approach specifically for Arabic, using an LSTM-based architecture to select among analyses produced by an existing morphological analyzer.

Most relevantly to our work, Sheyanova and Tyers [10] applied neural reranking to morphological disambiguation for Turkic languages, demonstrating that even with limited annotated data, neural models can effectively leverage the outputs of rule-based transducers. However, their approach did not fully exploit the potential of attention mechanisms in capturing the relationship between word context and morphological features.

Our work builds upon these foundations by applying an encoder-decoder architecture with Bahdanau attention specifically to the task of ranking morphological analyses for Yakut. Unlike fully generative approaches, we use the decoder not to generate analyses autoregressively, but to score candidate analyses produced by a rule-based transducer within the context of the sentence.





Subword tokenization also helps mitigate data sparsity.
Ideally, we aim to maintain a small vocabulary size to increase the frequency of observed subword units while ensuring that the resulting segments are linguistically meaningful.
Since vocabulary size directly determines the granularity of subword segmentation, careful design is essential to avoid both under-segmentation and over-segmentation.

To tokenize both the input and target sequences, we use the SentencePiece algorithm. For input sequences, our objective is for the tokenizer to approximate morphemes or syllables.

We define the desired granularity as an ideal distribution of subtoken counts over the set of unique word forms in the corpus.
This distribution is estimated by analyzing the annotated morphological structure of each word: we count the number of affixes in the corresponding morphological analysis and subtract one, since the first affix consistently corresponds to the lexical root type. A syllable segmentation algorithm is then applied to the lexical root to determine its number of syllables.

In cases where the root deviates from standard Yakut orthography and the syllabifier fails to segment it, the syllable count is estimated as the length of the root in characters divided by three, which reflects the average syllable length in Yakut.
The total number of desired subtokens for a given word is thus computed as the number of affixes (excluding the root marker) plus the number of syllables in the lexical root. For example:

- Input word: колхозтан
- Root: кол,хоз (2 syllables)
- Affixes: +N, +ABL (2 affixes → 2 − 1 = 1 counted affix)
- Total segments: 3

The ideal subtoken count distribution for the input tokenizer is then modeled as $ W = \{ len(w_1), len(w_2), ... len(w_n) \}$ , where each $ w_i $ is a unique word in the annotated corpus and $ len(w_i) $ is the estimated ideal number of subtokens (syllables + affixes) for that word.

We then train the input tokenizer with different vocabulary sizes and compare the resulting subtoken length distributions to the ideal distribution, selecting the vocabulary size that best approximates it. 

We explore a range of vocabulary sizes to train the tokenizer where the lower and upper bounds are given as ± 75% of the number of unique syllables, and compare the resulting subtoken length distributions to the ideal distribution to select the vocabulary size that best approximates it.

In the case of the target tokenizer, we want to train the algorithm to segment just the lexical roots, since the affixes are hardcoded as special tokens. Therefore, instead of using the a list of unique word forms from the annotated data, we use directly the list of roots from the morphological transducer's lexicon. We apply the same method used to segment the lexical roots in the input tokenizer.

The ideal subtoken count distribution for the target tokenizer is then modeled as $ R = \{ len(r_1), len(r_2), ... len(r_n) \}$ , where each $ r_i $ is a root in the morphological transducer's lexicon and $ len(r_i) $ is its estimated ideal number of subtokens (syllables) for that root.

To compare the distributions both for the input and target tokenizers with their respective ideal values, we use Earth Mover’s Distance.

Earth Mover’s Distance (EMD) provides a principled way to compare distributions over ordinal data such as token or syllable counts.
EMD reflects not only the amount of divergence but also the “distance” over which probability mass must be shifted, making it particularly well-suited to comparing segmentation granularity.



### 4.2 The Decoder 

The decoder is a unidirectional LSTM that generates a target sequence of subword tokens representing the output (candidate analyses) produced by the rule-base morphological transducer for the target word form.

Each output token is generated autoregressively, conditioned on the context vector $ c_t $, the previous hidden state, and the embedding of the previously generated token.
The output probability distribution over the target vocabulary is computed via a softmax layer:

$$ P(y_t|y_{<t}, S') = softmax(W_o[s_t, c_t, emb)(y_{t−1})] + b_o)  $$

where $ y_t $ is the $ t-th $ token in the target sequence.
The model is trained to minimize cross-entropy loss over the target sequence.




The sequence-to-sequence (seq2seq) paradigm, introduced by Sutskever et al. [8], transformed structured prediction tasks in natural language processing.
Their work demonstrated that encoder-decoder architectures could effectively map variable-length input sequences to output sequences, initially for machine translation. Bahdanau et al. [9] further advanced this approach by introducing an attention mechanism, enabling the decoder to focus on relevant parts of the input sequence during output generation. While these seq2seq models were designed for generative tasks, our approach adapts the encoder-decoder framework to rank pre-generated morphological analyses produced by a rule-based morphological transducer, rather than generating analyses from scratch.

The use of encoder-decoder models for ranking or scoring pre-generated linguistic analyses has been explored in related tasks. For example, in part-of-speech tagging, models have been employed to select the most likely tag sequence from candidate sets [e.g., Huang, Chen, & Zhao, 2015, using bidirectional LSTMs with CRFs for sequence-level scoring]. Similarly, in machine translation post-editing, encoder-decoder models have been used to score and rank potential corrections to translated sentences [e.g., Junczys-Dowmunt, Grundkiewicz, & Dwojak, 2016, using neural networks to score machine translation outputs]. These studies highlight the effectiveness of encoder-decoder architectures in learning contextual dependencies and selecting among constrained output candidates.

Building on these foundations, our model leverages an encoder-decoder RNN with attention to encode the input word and its sentential context, using the decoder to score pre-generated morphological analyses. This approach enables a sophisticated ranking function that surpasses the limitations of purely rule-based or frequency-based disambiguation methods, addressing the unique challenges of morphological disambiguation in Yakut.


By repurposing the encoder-decoder architecture for ranking rather than generation, we avoid the computational overhead of sampling or beam search typically required in generative tasks, making the approach efficient for practical applications.

Citation:
Neubig, G., Morishita, M., & Sakaguchi, K. (2018). Neural Reranking for Machine Translation. Proceedings of the Third Conference on Machine Translation: Research Papers, 257–263.

Relevance:

This paper discusses neural reranking for machine translation, where a model scores and ranks candidate translations. While it may not use an RNN encoder-decoder explicitly, the scoring mechanism is analogous to your task of ranking morphological analyses.
You would need to verify if the paper uses RNNs or adapt it to justify its relevance to your architecture.


Encoder-decoder architectures — with or without attention — have been widely adopted in morphological tasks, most often for morphological generation, such as generating inflected word forms from lemmas [11,12] or mapping between morphological feature sets and surface forms [13]. However, these models typically generate output from scratch and require full supervision.



### 3.4 Ranking Outputs

Instead of generating a target sequence autoregressively, we repurpose this model to rank candidate morphological analyses produced by an external rule-based morphological parser. This adaptation enables the model to disambiguate morphological forms by assigning preference scores to pre-generated analyses based on contextual appropriateness.

In the standard generative setup, the decoder autoregressively produces a target sequence by predicting one token at a time, conditioned on the encoder’s context embedding and previously generated tokens. For our ranking task, we modify the decoder’s role. Instead of generating a sequence, the decoder evaluates a set of candidate morphological analyses provided by the rule-based parser. Each candidate analysis is represented as a sequence of morphological tags (e.g., part-of-speech, case, number) corresponding to the ambiguous word in its sentential context.

To rank the candidates, we feed each candidate analysis into the decoder as a fixed target sequence. The decoder computes a probability score for the candidate by calculating the log-likelihood of the sequence given the encoder’s context embedding. Specifically, for a given input sentence $ S = \{w_1, w_2, ..., w_n\} $ and a candidate morphological analysis $ M = \{ t_1, t_2, ..., t_k \} $ for a target word, the model computes the conditional probability $ P (M|S) $, approximated as:

$$ P(M|S) = \prod^k_{i=1}  P(t_i | t_{<i}, S, \theta) $$

where $ t_{<i} $ ​denotes the preceding tags in the candidate analysis, and $ \theta $ represents the model parameters. The candidate with the highest probability is ranked as the most likely morphological analysis.

During inference, we batch-process the candidate analyses for each ambiguous word, computing their probabilities in parallel to improve efficiency.
