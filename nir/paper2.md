# Morphological Disambiguation for the Yakut Language

For interpretatibility: 007_02_0016 the imperative with beje


## Abstract


Morphological disambiguation entails selecting the correct morphological analysis for a word in context from multiple possible interpretations.
This paper addresses this task for Yakut (Sakha), a Turkic language spoken primarily in the Sakha Republic (Yakutia).
We employ an encoder-decoder recurrent neural network (RNN) with attention to rank candidate analyses generated by a rule-based morphological transducer, identifying the most contextually appropriate interpretation within a sentence.
Facing minimal morphologically annotated data, we experiment with data augmentation and subword tokenization strategies to mitigate data sparsity.
Our experiments demonstrate competitive disambiguation performance, advancing computational linguistics research on low-resource languages.


### Keywords
morphological disambiguation, Yakut language, encoder-decoder RNN, data augmentation, subword tokenization


## 1. Introduction

While subword tokenization algorithms have reduced the reliance on explicit morphological analysis in many natural language processing (NLP) tasks, such analysis remains essential for linguistic research and applications requiring fine-grained grammatical interpretation.

Morphological analysis maps a word’s surface form to its underlying morphological representation [1], yielding decompositions rooted in linguistic theory.
This process is highly language-specific and particularly complex for agglutinative languages like Yakut, where a single root can generate a vast number of word forms through sequential affixation [2].
The resulting morphological richness leads to data sparsity, which limits the effectiveness of purely data-driven methods such as neural networks—especially in low-resource languages like Yakut with limited annotated data.

Rule-based morphological transducers can model the productive affixation processes of agglutinative languages using a finite set of linguistic rules.
However, the mapping between surface forms and morphological analyses is often ambiguous: a single word form may correspond to multiple valid interpretations.
Morphological disambiguation—the task of selecting the correct analysis given the context—is therefore a crucial step in processing such languages [3].

This paper addresses morphological disambiguation for Yakut by evaluating an encoder-decoder model with attention, used to score candidate analyses generated by a rule-based morphological transducer [4].
Encoder-decoder architectures are well-suited for mapping input sequences to structured outputs while capturing contextual dependencies.
In our approach:

- The encoder generates context-aware embeddings for each token, incorporating rich linguistic features.
- The decoder uses these embeddings to predict structured morphological labels.

This hybrid approach combines the strengths of rule-based and data-driven methods.
The rule-based transducer ensures comprehensive coverage of morphologically valid forms, while the encoder-decoder model leverages contextual information to resolve ambiguities.

To address the challenge of data sparsity, we employ fine-grained subword tokenization and data augmentation techniques to expand the training data.
Our aim is to maximize disambiguation accuracy while minimizing dependence on large amounts of annotated data. We train and evaluate our model on a dataset of sequences with manually disambiguated morphological parses.

The remainder of this paper is organized as follows: Section 2 reviews the encoder-decoder architecture. Section 3 describes the data used for training and testing. Section 4 details the model design and implementation. Section 5 discusses the types of morphological ambiguities addressed, with evaluation results presented in Section 6. Section 7 concludes the paper.


## 2. Previous Work

To the best of our knowledge, there is no dedicated work on morphological disambiguation for the Yakut language.
Hakkani-Tür et al. [5] provide a comprehensive survey of morphological disambiguation approaches for Turkish up to 2017.
These methods range from early rule-based techniques—including both hand-crafted and machine-learned rules—to statistical and machine learning approaches such as hidden Markov models (HMMs), perceptron-based discriminative methods, and conditional random fields (CRFs). 
To address data sparsity issues in statistical models, several strategies have been proposed, such as simplifying output representations by retaining only the primary part-of-speech [6], focusing on the final inflectional group, or disambiguating roots and affixes separately [7].

Most of the approaches reviewed by Hakkani-Tür et al. precede the emergence of recurrent neural network (RNN) encoder-decoder architectures, which form the basis of our approach.
The encoder-decoder framework was introduced independently by Sutskever et al. [8] and Cho et al. [9] for machine translation.
While Sutskever et al. emphasized end-to-end sequence generation, Cho et al. also demonstrated the use of the model for scoring candidate translation pairs within a statistical machine translation system.
This application of encoder-decoder models for ranking or scoring, rather than generation, is particularly relevant to our work.

A significant advancement to the encoder-decoder architecture came with the introduction of the attention mechanism by Bahdanau et al. [10].
Attention enables the decoder to dynamically focus on relevant parts of the input sequence, thereby mitigating the limitations imposed by fixed-length context vectors in vanilla RNNs.
This mechanism has since become a standard component in sequence-to-sequence models, enhancing both interpretability and performance across a wide range of tasks.


## 3. Model Architecture

We employ an encoder-decoder model with an attention mechanism to score a list of pre-generated morphological analyses produced by a rule-based transducer for a given Yakut sentence $ S = [t_1, t_2, \dots, t_n] $, where each $ t_i $ represents a subword token. The objective is to disambiguate the morphological analysis of a target word by scoring a set of candidate analyses $  \{a_{i1}, a_{i2}, \dots, a_{im}\} $ provided by the transducer.

### 3.1 Encoder

The encoder is a bidirectional long short-term memory (LSTM) network that generates contextual representations of the input sentence. To focus on the target word, we enclose its corresponding subword tokens $ t_i, t_{i+1}, \dots, t_{i+k} $ with special tokens $ \text{<TARGET>} $ and $ \text{</TARGET>} $. The modified input sequence is thus:

$ S' = [t_1, \dots, t_{i-1}, \text{<TARGET>}, t_i, \dots, t_{i+k}, \text{</TARGET>}, t_{i+k+1}, \dots, t_n] $

Each subword token $ t_j $ (including the special tokens) is represented as a learned embedding, initialized randomly and optimized during training to capture Yakut-specific morphological and phonological patterns. The bidirectional LSTM processes $ S' $, computing forward $ (\overrightarrow{h_j}) $ and backward $ ( \overleftarrow{h_j} ) $ hidden states for each token, which are concatenated to form the contextual representation $ h_j = [\overrightarrow{h_j}, \overleftarrow{h_j}]$. The encoder outputs a sequence of hidden states:

$ H = [h_1, h_2, \dots, h_{n+2}] $,

where $ n+2 $ accounts for the additional $ \text{<TARGET>} $ and $ \text{</TARGET>} $ tokens.


### 3.2 Decoder with Bahdanau Attention

The decoder, a single-layer LSTM, scores each candidate morphological analysis $ a_{im} $ by processing the subword tokens of the target word and leveraging contextual information from the encoder's hidden states $ H $. The Bahdanau attention mechanism computes a context vector $ c_m $ for each candidate analysis by aligning the decoder's hidden state $ s_m $ with the encoder's hidden states $ H $ . Specifically, for each decoding step, the attention score for the $ j-th $ encoder hidden state is calculated as:

$ e_{m,j} = v_a^\top \tanh(W_a s_m + U_a h_j) $,

where $ v_a $, $ W_a $, and $ U_a $ are learned parameters, and $ s_m $ is the decoder's hidden state at step $ m $. The attention weights are obtained via a softmax operation:

$ \alpha_{m,j} = \frac{\exp(e_{m,j})}{\sum_{k=1}^{n+2} \exp(e_{m,k})} $, 

and the context vector is computed as:

$ c_m = \sum_{j=1}^{n+2} \alpha_{m,j} h_j $

The context vector $ c_m $ is concatenated with the decoder's hidden state $ s_m $ and passed through a feed-forward layer to produce a score for each candidate analysis $ a_{im} $. A softmax layer then converts these scores into a probability distribution over the candidate analyses, enabling the model to select the most likely morphological analysis.

The decoder operates non-autoregressively, evaluating each candidate analysis independently without conditioning on previously scored candidates.



## 4. Tokenizer and Model Training

The training set consist of sequences with morphological annotations that have been manually disambiguated [SOURCE].

Since the training data is small, we apply some strategies to augment the training samples and we resource to a fine-grain subword tokenization to produce less sparse observations.

Prevent the model from overfiting, memorizing the training data, instead of learning patterns from it.


### 4.1 Data Augmentation

A extra benefit of encompasing the subtokens between the special tokens than encode the target word is data augmentation. Each time the target markers window slides along the sequence, the encoder produces a slightly different context embedding for predicting the target tokens.

Let $ D = \{S_1, S_2, ..., S_N\} $ denote a corpus consisting of $ N $ morphological annotated sequences. Each sequence $ S_i $ contains $ I_i $ input words, denoted as $ (i_{i1}, i_{i2}, ... i_{iI_i} ) $ and $T_i$ corresponding target analyses, denoted as $ (t_{i1}, t_{i2}, ... t_{iT_i} ) $. For each word $ i_{ij} $ , we generate a distinct training example where the target output is $ t_{ij} $ . Therefore, the total number of training examples $ M $ generated from the corpus is:

$$ M = \sum^N_{i=1} I_i $$

Each sequence is then augmented by the number of word forms it contains, which helps to expand the small size of our manually disambiguated corpus (12,000 sentences).

We train the model on unambiguous analyses as well. These cases don’t directly contribute to the model’s ability to discriminate between competing analyses. However, this allows the encoder to learn robust, context-aware embeddings for a broader range of words and sentences. Even if a word is unambiguous, its context (surrounding words, syntactic structure) contributes to the encoder’s ability to model contextual dependencies, which is critical for disambiguating ambiguous cases.

Moreover, unambiguous parses act as “easy” positive examples, providing stable gradients during training. This can help balance the learning process, especially when negative samples (incorrect analyses) introduce noise or complexity.


### 4.2 Subword Tokenization

We use the SentencePiece algorithm [1] to tokenize both input and target sequences, aiming to approximate morphemes or syllables for input sequences and segment lexical roots for target sequences.

For input sequences, the tokenizer is designed to segment words into subwords reflecting their morphological structure. The ideal subword count for each word is estimated by analyzing its annotated morphological structure: the number of affixes (excluding the lexical root) plus the number of syllables in the root. For roots, syllable counts are determined using a syllable segmentation algorithm. If the algorithm fails (e.g., for roots deviating from standard Yakut orthography), the syllable count is approximated as the root’s character length divided by three, reflecting the average syllable length in Yakut. For example:

- Input word: колхозтан
- Root: кол,хоз (2 syllables)
- Affixes: +N, +ABL (2 affixes, 1 counted after excluding the root marker)
- Total subwords: 3

The ideal subword count distribution for the input tokenizer is modeled as $ W = \{ len(w_1), len(w_2), ... len(w_n) \} $ , where $ w_i $ is a unique word in the corpus, and $ len(w_i) $ is its estimated subword count (syllables + affixes). We train the input tokenizer across a range of vocabulary sizes (±75% of the number of unique syllables in the corpus) and select the size that minimizes the Earth Mover’s Distance (EMD) [2] between the resulting subword count distribution and the ideal distribution.

For target sequences, the tokenizer segments only lexical roots, as affixes are represented as hardcoded special tokens. The ideal subword count distribution is modeled as $ R = \{ len(r_1), len(r_2), ... len(r_n) \}$ , where $ r_i $ is a root the morphological transducer's lexicon, and $ len(r_i) $ is its syllable count, estimated as described above. The target tokenizer is trained similarly, selecting the vocabulary size that minimizes EMD between the subword count distribution and the ideal distribution.

EMD is used to compare subword count distributions because it accounts for both the magnitude and ordinal distance of divergences, making it suitable for evaluating segmentation granularity. This methodology ensures robust tokenization, enabling the RNN to capture morphological and semantic patterns effectively.

Additionally, this approach produces subword units of similar length in both input and target tokenizers, facilitating the interpretability of the RNN’s attention mechanism by enabling consistent analysis of attention weights across sequences.


### 4.3 Training Hyperparameters

Hyperparameters, such as the number of LSTM layers, hidden state size, and attention heads, are tuned via cross-validation on a held-out development set.

Use teacher forcing during training, where the decoder is fed the entire candidate sequence $ M $ (correct or incorrect) as input, rather than predicting tokens. This ensures the decoder evaluates the likelihood of the provided sequence.


The model was trained with the following hyperparameters:

- Embedding Size: 128 for subword and marker token embeddings, learned during training.
- Hidden State Size: 128 for encoder and decoder LSTMs, balancing expressiveness and dataset size.
- Optimizer: Adam with a learning rate of 0.001.
- Dropout: 0.3 to prevent overfitting.
- Batch Size: 32, optimized for GPU training.

The training corpus consists of 1,200 manually disambiguated Yakut sentences, tokenized into subword units using a SentencePiece model trained on
the corpus.
Each sentence was preprocessed to generate modified sequences with <TARGET> and </TARGET> tokens enclosing each word’s subword tokens, paired with the gold-standard candidate analysis from the transducer.
Training was conducted for 15 epochs, with early stopping based on validation set performance.


SOFTMAX

Since the true sequence is always present among the candidate sequences, we can simplify the training process by using the index of the true sequence within candidate_seqs as the target for a cross-entropy loss with softmax, as you suggested. This approach treats the ranking task as a classification problem, where the model assigns probabilities to each candidate sequence and is trained to maximize the probability of the true sequence. Below, I’ll provide a complete solution tailored to this setup, including data preparation, loss function, training loop, and evaluation, leveraging the EncoderDecoderRanker model from the previous implementation.


CAVEATS


This approach requires re-encoding the input sentence for each word to disambiguate.

Benefit: This increases the amount of training data, which is valuable for a low-resource language like Yakut, potentially helping the model generalize better.
Downside: It increases preprocessing complexity and memory requirements, as you’re storing multiple versions of each sentence.

For each sentence in the manually disambiguated Yakut corpus, you’d generate multiple training examples—one for each word in the sentence.
This increases the amount of training data, which is valuable for a low-resource language like Yakut, potentially helping the model generalize better.



## 5. Types of Morphological Ambiguity

Morphological ambiguity stems from diverse sources.
Hakkani-Tur [SOURCE] distinguishes for Turkish between part-of-speech ambiguity, segmentation ambiguity, morphographemic processes, and homograph morphemes.
All these types are also observed in the Yakut language, each posing different degrees of difficulty to solve.

For example, part-of-speech ambiguity can have its origin in the lexicon, when a lexical root is associated with more than one primary stem.
For instance, the root *санаа* can both be interpreted as a verbal stem (the second person imperative form of “to think”) or as a nominal stem (the singular nominative case of “thought”).
Affixation often helps to solve this type of lexical ambiguity, since stem types introduce restrictions on the affixes that can be attached to them.

Furthermore, lexicalized derivations, where derived forms develop independent and often idiomatic meanings, introduce additional ambiguities... [TUR].

Some monosyllabic adjectives coincide with verbs both in form and in content.
There are bases that are identical in sound composition, but completely different in meaning.
They are grammatical homonyms [§255, 161].

Segmentation ambiguity is very pervasive, since word forms often can be segmented in multiple different ways. This phenomenon is widespread in Yakut for disyllabic verbal roots that have a “fossilized” productive affix in the last syllable.
Read §500 about dead roots in reflexive verbs.

For example, the form *кэбис* can be analyzed as a lexical root with the sense of “to throw, toss, pitch, cast” or as *кэп* “push away, shove someone/something away“ + the reciprocal affix *-ис*.

Segmentation ambiguity also occurs with the combination of some affixes.
For example, it is observed systematically among the possessive cases in relation to the plural.
For instance, the form доруобуйаларыгар can be segmented both as:

доруобуйа+N+POSS.3PL+DAT
доруобуйа+N+PL+POSS.3SG+DAT

The root *доруобуйа* “health” can be interpreted as singular or plural.


In other cases, ambiguity can be introduced by morphographemic processes.
For example, the verbal roots *көп* “to rise, to float to the surface” and *көт* “to fly” have the same past participle form *көппүт*.
By regressive assimilation, an affix starting with a labial consonant like the past participle *-быт*, forces the final dental voiceless stop *(т)* in the stem көт to changes to a bilabial voiced stop *(п)*.
Because *көт* and *көп* are verbs, *көппүт* can only be disambiguated semantically.

Morphographemic processes render different interpretations of a surface form.
For example, *барыах* is analyzed as a verbal root *бар* “to go, leave, depart” with the future participle *-ыах*:

бар+V+PART_FUT

However, since affixes starting with vowels or diphthongs cause the deletion of long consonants in stem endings [SOURCE], the lexical root can also be interpreted as the verbal root *бараа* “to spend”:

бараа+V+PART_FUT

Yakut also has homograph morphemes.
For example, the predicate and possessive affixes in Yakut have the same form for the first and second person of the plural (*-быт*, *-гыт* and their allomorphs) [Ubryatova, 311].
Another example is observed for *-нан* and its allomorphs, which represents the instrument both in its plain and in its possessive variants.
When homograph affixes have similar grammatical function, syntax is not sufficient to disambiguate them.

Considering these various sources of ambiguity, the encoder-decoder model's successful processing of Yakut morphology hinges upon its ability to discern and resolve these key types – stemming from lexical variations, structural segmentation, phonological interactions, and formal overlaps – solely through the statistical regularities and contextual dependencies learned from the provided training data.


## 6. Evaluation


The testing set contains 300 sentences compiled from a work on Yakut morpholgy [SOURCE], following the same methodology.

This is particularly effective for Yakut’s local syntactic dependencies, such as agreement in case or number, where surrounding syllables provide critical cues.




## 7. Conclusions and Further Work

We implemented a encoder-decoder model with attention to score candidate morphological analyses provided by the parser.

This hybrid approach leverages the parser’s comprehensive morphological coverage and the LSTM’s contextual sensitivity.

The architecture is designed to leverage limited annotated data while effectively capturing contextual dependencies critical for disambiguating affixes and lexical roots.


The rule-based morphological parser generates a set of possible analyses for each ambiguous word based on predefined linguistic rules and a lexicon. These analyses often include all morphologically valid forms, without considering contextual constraints. Our LSTM-based model complements the parser by leveraging contextual information to rank these analyses. For each ambiguous word in a sentence, the parser outputs a set of candidate analyses, which are then scored by the LSTM model. The highest-scoring analysis is selected as the disambiguated output.


Batch Candidates
When you batch the candidates, you are essentially passing multiple sequences through the encoder-decoder architecture in parallel, rather than one at a time. This approach takes advantage of parallelization (either on the CPU or GPU), reducing the overall time it takes to evaluate all candidates.

Conditional scoring for structured prediction


# References

[1] Sproat R. (1992). Morphology and Computation. ACL-MIT Press series in natural-language processing. 1992 Massachusetts Institute of Technology.

[2] Oflazer K., Saraçlar M. eds (2017) Turkish and Its Challenges for Language and Speech Processing. In: Turkish Natural Language Processing.

[3] Hakkani-Tür D., Oflazer K., Tür G. (2002). Statistical Morphological Disambiguation for Agglutinative Languages, 2002.

[4] Cortegoso Vissio N., Khokhlova M.A. (2024). Three-Stage Morphological Pipeline for Word Form Analysis in Yakut. Conference: 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE), pp. 1860-1864.

[5] Hakkani-Tür D.Z., Saraçlar M., Tür G., Oflazer K., Yuret D. (2017) Morphological disambiguation for Turkish. In: Turkish Natural Language Processing.

[6] Ehsani R., Alper M.E., Eryig ̆it G., Adalı E (2012) Disambiguating main POS tags for Turkish. In: Proceedings of the 24th conference on computational linguistics and speech processing, Chung-Li.

[7] Hakkani-Tür D.Z., Oflazer K., Tür G. (2002) Statistical morphological disambiguation for agglutinative languages. Comput Hum 36(4):381–410.

[8] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.

[9] Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. arXiv:1406.1078.

[10] Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. ICLR 2015.

[11] Kann, K., & Schütze, H. (2016). MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection. In Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology.

[12] Cotterell, R., & Heigold, G. (2017). Cross-lingual Character-level Neural Morphological Tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).

[13] Aharoni, R., & Goldberg, Y. (2017). Morphological Inflection Generation with Hard Monotonic Attention. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).







[2] Rubner, Y., Tomasi, C., & Guibas, L. J. (2000). The Earth Mover’s Distance as a metric for image retrieval. International Journal of Computer Vision, 40(2), 99–121.


[] Morphological disambiguation of Turkish text with perceptron algorithm.




[6] Hulden, M., & Francom, J. (2012). Boosting statistical tagger accuracy with simple rule-based grammars. LREC 2012.
[7] Silfverberg, M., Ruokolainen, T., Lindén, K., & Kurimo, M. (2016). FinnPos: An open-source morphological tagging and lemmatization toolkit for Finnish. Language Resources and Evaluation, 50(4), 863-878.
[8] Cotterell, R., Müller, T., Fraser, A., & Schütze, H. (2015). Labeled morphological segmentation with semi-Markov models. CoNLL 2015.
[9] Zalmout, N., & Habash, N. (2019). Adversarial multitask learning for joint multi-feature and multi-dialect morphological modeling. ACL 2019.
[10] Sheyanova, M., & Tyers, F. M. (2017). Annotation schemes in North Slavic corpora. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing.


## JUNK


Linguistic representation) tends to be structured, hierarchical, and highly dependent on linguistic theory, not just data frequency.



## 2. Previous Work



Sequence-to-sequence (seq2seq) learning was introduced by Sutskever et al. [[3]](#references), proposing an encoder-decoder architecture for mapping variable-length input sequences to variable-length output sequences.
Their model, based on recurrent neural networks, laid the foundation for many subsequent developments in machine translation and other structured prediction tasks.
The introduction of attention mechanisms by Bahdanau et al. [[4]](#references) further enhanced the seq2seq framework by allowing the decoder to selectively focus on relevant parts of the input sequence during generation, significantly improving performance on long and complex sequences.

Traditionally, encoder-decoder architectures have been used as generative models, producing outputs autoregressively based on previously generated tokens and the internal encoder representation.
However, several works have explored ways to adapt these models for discriminative tasks. Ranzato et al. [[5]](#references) proposed sequence-level training objectives for recurrent networks, showing that training models to rank entire candidate sequences can lead to better performance than token-level prediction. Similarly, Shen et al. [[6]](#references) introduced discriminative training strategies that enable models to directly optimize over candidate rankings rather than generation.
In information retrieval, MacAvaney et al. [[7]](#references) demonstrated that neural models with contextualized embeddings could be effectively trained to score and rank candidate outputs. Relatedly, Bahdanau et al. [[8]](#references) showed that attention-based models could be adapted for speech recognition tasks by discriminatively selecting among competing hypotheses during decoding.

While most work has focused on training models to generate or rank sequences in high-resource settings, relatively few efforts have investigated adapting encoder-decoder models for discriminative scoring in low-resource contexts, where only minimal annotated data is available.
Our work builds on these ideas by proposing a encoder-decoder architecture that scores candidate morphological analyses of a given word form, rather than generating them, allowing effective morphological disambiguation for Yakut despite the limited training data.



[3] Sutskever, I., Vinyals, O., & Le, Q.V. (2014). Sequence to Sequence Learning with Neural Networks. ArXiv.
[4] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate.
[5] Ranzato M.,  Chopra S., Auli M., Zaremba W. (2016). Sequence Level Training with Recurrent Neural Networks. International Conference on Learning Representations (ICLR).
[6] Shen Y., He X., Gao J., Deng L., Mesnil G. (2015). Training Neural Networks with Noisy Labels via Bootstrapping. International Conference on Learning Representations (ICLR) Workshop.
[7] MacAvaney S., Yates A., Cohan A., Goharian N. (2019). CEDR: Contextualized Embeddings for Document Ranking. Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval.
[8] Bahdanau D., Chorowski J., Serdyuk D., Brakel P., Bengio Y. (2016). End-to-End Attention-Based Large Vocabulary Speech Recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).




- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.




Morphological disambiguation is typically harder for agglutinative languages, such as Yakut for the following reasons:

- High Morpheme Density: A single word can encode multiple grammatical features, requiring disambiguation of overlapping or nested morpheme boundaries.

- Ambiguity in Segmentation: Morphemes may have homophonous forms (e.g., a suffix might mark a different morphological feature depending on context).

- Sparse Data: Rare morpheme combinations may not appear in training data, complicating statistical disambiguation.

The last point is particulary challenging for applying machine learning models (e.g., neural networks, CRFs) to the disambiguation task, as they rely on frequency statistics.
The training corpus may not contain enough examples of certain word forms or morpheme combinations to reliably estimate their probabilities.



The bootleneck is in the decoder part.




to select correct output among candidates in the context of the sequence.


- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.







A morphological transducer generates an analysis representation from a word's surface form, capturing its grammatical structure and lexical properties.


Morphological disambiguation is the task of selecting the correct sequence of morphological parses corresponding to a sequence of words, from the set of possible parses for those word forms [1].

This disambiguation is particularly challenging for agglutinative languages like Yakut (Sakha), where a single root can generate numerous distinct word forms through sequential affixation of grammatical morphemes.



PREVIOUS WORK

This hybrid approach of combining rule-based transducers with statistical or neural ranking has been explored in various contexts. Hulden and Francom [6] demonstrated the effectiveness of combining finite-state transducers with statistical methods for morphological disambiguation in Spanish. Similarly, Silfverberg et al. [7] employed neural networks to rerank the outputs of a morphological generator, showing significant improvements over pure rule-based systems.

For morphologically rich languages, Cotterell et al. [8] explored neural morphological disambiguation as a structured prediction task, while Zalmout and Habash [9] developed a neural approach specifically for Arabic, using an LSTM-based architecture to select among analyses produced by an existing morphological analyzer.

Most relevantly to our work, Sheyanova and Tyers [10] applied neural reranking to morphological disambiguation for Turkic languages, demonstrating that even with limited annotated data, neural models can effectively leverage the outputs of rule-based transducers. However, their approach did not fully exploit the potential of attention mechanisms in capturing the relationship between word context and morphological features.

Our work builds upon these foundations by applying an encoder-decoder architecture with Bahdanau attention specifically to the task of ranking morphological analyses for Yakut. Unlike fully generative approaches, we use the decoder not to generate analyses autoregressively, but to score candidate analyses produced by a rule-based transducer within the context of the sentence.





Subword tokenization also helps mitigate data sparsity.
Ideally, we aim to maintain a small vocabulary size to increase the frequency of observed subword units while ensuring that the resulting segments are linguistically meaningful.
Since vocabulary size directly determines the granularity of subword segmentation, careful design is essential to avoid both under-segmentation and over-segmentation.

To tokenize both the input and target sequences, we use the SentencePiece algorithm. For input sequences, our objective is for the tokenizer to approximate morphemes or syllables.

We define the desired granularity as an ideal distribution of subtoken counts over the set of unique word forms in the corpus.
This distribution is estimated by analyzing the annotated morphological structure of each word: we count the number of affixes in the corresponding morphological analysis and subtract one, since the first affix consistently corresponds to the lexical root type. A syllable segmentation algorithm is then applied to the lexical root to determine its number of syllables.

In cases where the root deviates from standard Yakut orthography and the syllabifier fails to segment it, the syllable count is estimated as the length of the root in characters divided by three, which reflects the average syllable length in Yakut.
The total number of desired subtokens for a given word is thus computed as the number of affixes (excluding the root marker) plus the number of syllables in the lexical root. For example:

- Input word: колхозтан
- Root: кол,хоз (2 syllables)
- Affixes: +N, +ABL (2 affixes → 2 − 1 = 1 counted affix)
- Total segments: 3

The ideal subtoken count distribution for the input tokenizer is then modeled as $ W = \{ len(w_1), len(w_2), ... len(w_n) \}$ , where each $ w_i $ is a unique word in the annotated corpus and $ len(w_i) $ is the estimated ideal number of subtokens (syllables + affixes) for that word.

We then train the input tokenizer with different vocabulary sizes and compare the resulting subtoken length distributions to the ideal distribution, selecting the vocabulary size that best approximates it. 

We explore a range of vocabulary sizes to train the tokenizer where the lower and upper bounds are given as ± 75% of the number of unique syllables, and compare the resulting subtoken length distributions to the ideal distribution to select the vocabulary size that best approximates it.

In the case of the target tokenizer, we want to train the algorithm to segment just the lexical roots, since the affixes are hardcoded as special tokens. Therefore, instead of using the a list of unique word forms from the annotated data, we use directly the list of roots from the morphological transducer's lexicon. We apply the same method used to segment the lexical roots in the input tokenizer.

The ideal subtoken count distribution for the target tokenizer is then modeled as $ R = \{ len(r_1), len(r_2), ... len(r_n) \}$ , where each $ r_i $ is a root in the morphological transducer's lexicon and $ len(r_i) $ is its estimated ideal number of subtokens (syllables) for that root.

To compare the distributions both for the input and target tokenizers with their respective ideal values, we use Earth Mover’s Distance.

Earth Mover’s Distance (EMD) provides a principled way to compare distributions over ordinal data such as token or syllable counts.
EMD reflects not only the amount of divergence but also the “distance” over which probability mass must be shifted, making it particularly well-suited to comparing segmentation granularity.



### 4.2 The Decoder 

The decoder is a unidirectional LSTM that generates a target sequence of subword tokens representing the output (candidate analyses) produced by the rule-base morphological transducer for the target word form.

Each output token is generated autoregressively, conditioned on the context vector $ c_t $, the previous hidden state, and the embedding of the previously generated token.
The output probability distribution over the target vocabulary is computed via a softmax layer:

$$ P(y_t|y_{<t}, S') = softmax(W_o[s_t, c_t, emb)(y_{t−1})] + b_o)  $$

where $ y_t $ is the $ t-th $ token in the target sequence.
The model is trained to minimize cross-entropy loss over the target sequence.




The sequence-to-sequence (seq2seq) paradigm, introduced by Sutskever et al. [8], transformed structured prediction tasks in natural language processing.
Their work demonstrated that encoder-decoder architectures could effectively map variable-length input sequences to output sequences, initially for machine translation. Bahdanau et al. [9] further advanced this approach by introducing an attention mechanism, enabling the decoder to focus on relevant parts of the input sequence during output generation. While these seq2seq models were designed for generative tasks, our approach adapts the encoder-decoder framework to rank pre-generated morphological analyses produced by a rule-based morphological transducer, rather than generating analyses from scratch.

The use of encoder-decoder models for ranking or scoring pre-generated linguistic analyses has been explored in related tasks. For example, in part-of-speech tagging, models have been employed to select the most likely tag sequence from candidate sets [e.g., Huang, Chen, & Zhao, 2015, using bidirectional LSTMs with CRFs for sequence-level scoring]. Similarly, in machine translation post-editing, encoder-decoder models have been used to score and rank potential corrections to translated sentences [e.g., Junczys-Dowmunt, Grundkiewicz, & Dwojak, 2016, using neural networks to score machine translation outputs]. These studies highlight the effectiveness of encoder-decoder architectures in learning contextual dependencies and selecting among constrained output candidates.

Building on these foundations, our model leverages an encoder-decoder RNN with attention to encode the input word and its sentential context, using the decoder to score pre-generated morphological analyses. This approach enables a sophisticated ranking function that surpasses the limitations of purely rule-based or frequency-based disambiguation methods, addressing the unique challenges of morphological disambiguation in Yakut.


By repurposing the encoder-decoder architecture for ranking rather than generation, we avoid the computational overhead of sampling or beam search typically required in generative tasks, making the approach efficient for practical applications.

Citation:
Neubig, G., Morishita, M., & Sakaguchi, K. (2018). Neural Reranking for Machine Translation. Proceedings of the Third Conference on Machine Translation: Research Papers, 257–263.

Relevance:

This paper discusses neural reranking for machine translation, where a model scores and ranks candidate translations. While it may not use an RNN encoder-decoder explicitly, the scoring mechanism is analogous to your task of ranking morphological analyses.
You would need to verify if the paper uses RNNs or adapt it to justify its relevance to your architecture.


Encoder-decoder architectures — with or without attention — have been widely adopted in morphological tasks, most often for morphological generation, such as generating inflected word forms from lemmas [11,12] or mapping between morphological feature sets and surface forms [13]. However, these models typically generate output from scratch and require full supervision.



### 3.4 Ranking Outputs

Instead of generating a target sequence autoregressively, we repurpose this model to rank candidate morphological analyses produced by an external rule-based morphological parser. This adaptation enables the model to disambiguate morphological forms by assigning preference scores to pre-generated analyses based on contextual appropriateness.

In the standard generative setup, the decoder autoregressively produces a target sequence by predicting one token at a time, conditioned on the encoder’s context embedding and previously generated tokens. For our ranking task, we modify the decoder’s role. Instead of generating a sequence, the decoder evaluates a set of candidate morphological analyses provided by the rule-based parser. Each candidate analysis is represented as a sequence of morphological tags (e.g., part-of-speech, case, number) corresponding to the ambiguous word in its sentential context.

To rank the candidates, we feed each candidate analysis into the decoder as a fixed target sequence. The decoder computes a probability score for the candidate by calculating the log-likelihood of the sequence given the encoder’s context embedding. Specifically, for a given input sentence $ S = \{w_1, w_2, ..., w_n\} $ and a candidate morphological analysis $ M = \{ t_1, t_2, ..., t_k \} $ for a target word, the model computes the conditional probability $ P (M|S) $, approximated as:

$$ P(M|S) = \prod^k_{i=1}  P(t_i | t_{<i}, S, \theta) $$

where $ t_{<i} $ ​denotes the preceding tags in the candidate analysis, and $ \theta $ represents the model parameters. The candidate with the highest probability is ranked as the most likely morphological analysis.

During inference, we batch-process the candidate analyses for each ambiguous word, computing their probabilities in parallel to improve efficiency.
