# Morphological Disambiguation for the Yakut Language

For interpretatibility: 007_02_0016 the imperative with beje


## Abstract

Morphological disambiguation entails selecting the correct morphological analysis for a word in context from multiple possible interpretations.
This paper addresses this task for Yakut (Sakha), a morphologically rich Turkic language spoken primarily in the Sakha Republic (Yakutia).
We employ an encoder-decoder RNN with attention to rank the outputs of a rule-based morphological transducer and identify the most contextually appropriate interpretation within a sentence.
For that porpuse we employ data agmentation and a subword tokenization strategy to get the most of limited annotated data.
Our experiments demonstrate that competitive performance can be achieved, advancing computational linguistics research on low-resource languages.

### Keywords
morphological disambiguation, discriminative model

## 1. Introduction

Morphological disambiguation —the task of selecting the correct morphological analysis for a given word form— is a crucial step in natural language processing (NLP), especially for morphologically rich languages.

Although subword tokenization algorithms have reduced the reliance on explicit morphological analysis in many NLP tasks, morphological disambiguation remains essential for linguistic research and for applications requiring fine-grained grammatical interpretation.

This task presents considerable challenges for languages with complex morphology, such as those in the Turkic family.
Hakkani-Tür [1] provides a comprehensive overview of disambiguation methods ranging from rule-based systems to statistical approaches, with a focus on Turkish —the most well-resourced Turkic language— and highlights the intrinsic difficulties involved.

A major obstacle to statistical methods, as noted by Hakkani-Tür [ibid.], is data sparsity.
Agglutinative languages like Yakut, where a single root can generate numerous distinct word forms through sequential affixation limits the effectiveness of purely data-driven approaches, as many word forms would not even be observable in very large corpora.

In this paper, we address the problem of morphological disambiguation for Yakut by evaluating the performance of an encoder-decoder model used to rank candidate analyses generated by a rule-based morphological transducer [2], applied within sentence context.

Since applying a purely neural approach 

A rule-based morphological transducer can model this very productive affixation process from a finite set of rules.





Our goal is to maximize disambiguation accuracy while minimizing dependence on extensive annotated training data.

Encoder-decoder architectures have shown strong performance in tasks that require mapping input sequences to structured outputs while capturing contextual dependencies. In such models:
- The encoder generates context-aware embeddings for each token, capturing rich linguistic features.
- The decoder uses these embeddings to produce structured linguistic labels for each token.

For Yakut, the encoder can be trained effectively on large collections of unlabeled token sequences. However, the decoder presents a greater challenge.

The output of the decoder is a structured morphological representation grounded in linguistic theory, and thus highly language-specific.
Therefore, the decoder must be trained on morphological annotated data, which is scarce for low-resource languages like Yakut.

We currently have at our disposal a collection of sequences with manually disambiguated morphological parses [3].
To reduce data sparsity and make the input more suitable for the encoder-decoder model, we apply a syllable-based subword tokenization algorithm.

The remainder of this paper is organized as follows: Section 2 presents a general overview of the encoder-decoder architecture. Section 3 outlines the data distribution used for training and testing. Section 4 describes the model architecture and implementation details. Section 5 details the types of morphological ambiguities targeted by the model, which are evaluated in Section 6. Finally, Section 7 provides our conclusions.


## 2. Previous Work

The sequence-to-sequence (seq2seq) paradigm, formalized by Sutskever et al. [4], revolutionized approaches to structured prediction tasks in natural language processing.
Their seminal work demonstrated how encoder-decoder architectures could effectively map between variable-length input and output sequences for machine translation.
This approach was further enhanced by Bahdanau et al. [5], who introduced an attention mechanism allowing the decoder to focus on different parts of the input sequence when generating each element of the output sequence.

While these foundational seq2seq models are primarily designed for generative tasks where outputs are produced autoregressively, our approach differs in that the potential output morphological analyses are provided by a rule-based morphological transducer.
Therefore, rather than generating morphological analyses from scratch, we leverage machine learning to rank these existing candidate analyses based on their likelihood given the input word form and its context.

The application of encoder-decoder models for ranking or scoring pre-generated linguistic analyses has been explored in various contexts.
For instance, in part-of-speech tagging disambiguation, models have been used to select the best tag sequence from a set of candidates.
While not strictly encoder-decoder in all cases, architectures leveraging similar principles of contextual encoding and scoring have been successful [e.g., Huang, Chen, & Zhao, 2015 (for BiLSTMs with CRF for tagging, demonstrating sequence-level scoring); Collobert et al., 2011 (for neural network based tagging)].
For approaches more directly using encoder-decoder concepts for ranking in tagging or similar sequence labeling tasks, you might look for work that re-ranks n-best lists of tags using a sequence model.

Similarly, in machine translation post-editing, encoder-decoder models have been used to score and rank potential corrections to automatically translated sentences [e.g., Cho et al., 2014 (while focused on generation, it highlights the power of encoder-decoder for sequence understanding, which is relevant for scoring edits); Junczys-Dowmunt, Grundkiewicz, & Dwojak, 2016 (using neural networks to score MT output, a related ranking task)].

These studies demonstrate the utility of encoder-decoder architectures in learning contextual dependencies and making informed decisions among a set of candidate outputs, even when the output space is constrained or pre-defined. By encoding the input word and its surrounding context, and then using the decoder (potentially with attention over the input) to score each pre-generated morphological analysis, our model aims to learn a sophisticated ranking function that goes beyond the capabilities of purely rule-based or frequency-based disambiguation methods.



## 3. Morphologicaly Annotated Data




This situation worsen even more when we need annotated data.

The maping between a word form with its analysis is not one-to-one. 





Data with morphological annotations is small.

Present statistics.



As testing set 


Two strategies to get the most of the currenlty data with morphological annotations at our disposal.

- subword tokenization to approximate morphemes.
- an data agumentation



## 4. Model Architecture

For a given Yakut sentence $ S = [t_1, t_2, . . . , t_n] $, where each $ t_i $ represents a subword token, the goal is to disambiguate the morphological analysis of a specific word form (the target word) by ranking a set of candidate analyses provided by a rule-based morphological transducer (e.g., $ {a_{i1}, a_{i2}, ..., a_{im}} $.

We frame this as a sequence-to-sequence (seq2seq) task, where an
The target word’s subword tokens tokens to guide the attention mechanism.

The model is an encoder-decoder RNN with a Bahdanau attention mechanism [1], designed to operate on subword tokens to reduce sparsity in Yakut’s morphologically rich, agglutinative language.



### 4.1 The Encoder

The encoder is a bidirectional long short-term memory (LSTM) network that produces a contextual representation of the input sentence, where the subwords tokens that correspond to the target word form are enclosed between the special tokens  \<TARGET> and \</TARGET>. 

For a target word spanning subword tokens $ t_i , t_{i+1}, . . . , t_{i+k} $, the input sequence is encoded as $ S' = [t_1, . . . , t+{i−1}, \text{<TARGET>}, t_i, . . . , t_{i+k}, \text{</TARGET>}, t_{i+k+1}, . . . , t_n] $.

Each subword token $ t_j $ (including marker tokens) is represented as a learned embedding, initialized randomly and optimized during training to capture morphological and phonological patterns specific to Yakut. The bidirectional LSTM computes forward and backward hidden states, concatenated to form $ h_j = [
−→h_j, ←−h_j] $, yielding a sequence of hidden states $ H = [h_1, h_2, . . . , h_{n+2}] $.


### 4.2 The Decoder 

The decoder is a unidirectional LSTM that generates a target sequence of subword tokens representing the output (candidate analyses) produced by the rule-base morphological transducer for the target word form.

Each output token is generated autoregressively, conditioned on the context vector $ c_t $, the previous hidden state, and the embedding of the previously generated token.
The output probability distribution over the target vocabulary is computed via a softmax layer:

$$ P(y_t|y_{<t}, S') = softmax(W_o[s_t, c_t, emb)(y_{t−1})] + b_o)  $$

where $ y_t $ is the $ t-th $ token in the target sequence.
The model is trained to minimize cross-entropy loss over the target sequence.


### 4.3 The Attention Mechanism

We use Bahdanau attention [1] to compute context vectors that prioritize relevant subword tokens for generating the target sequence. For each decoder step $ t $, alignment scores are calculated between the decoder’s hidden state $ s_t $ and the encoder’s hidden states $ h_j $:


$$ e_{tj} = v^T_\alpha \tanh(W_\alpha s_t + U_\alpha h_j) (1) $$

$$ \alpha_{tj} = \text{softmax}(e_{tj}) (2) $$ 


The context vector is $ c_t = \sum_j \alpha_{tj} h_j $.

The \<TARGET> and \</TARGET> tokens in the input guide the attention mechanism to focus on the target word’s subword tokens and their context.

The <TARGET> tokens provide a clear signal to the model about which word to disambiguate, which is particularly helpful with limited training data. This can guide the attention mechanism to focus on the target word and its immediate context, improving disambiguation accuracy for Yakut’s agglutinative morphology (e.g., resolving case or number ambiguities).

To improve performance, we implement the Bahdanau attention mechanism, which allows the decoder to dynamically focus on different parts of the input sequence at each time step.
This helps overcome the bottleneck problem of traditional sequence-to-sequence models, which rely solely on the encoder’s final hidden state.


### 5. Tokenizer and Model Training

For a corpus with N sentences and an average of L words per sentence, we generate N × L training examples, each with the marker tokens enclosing a different word’s syllables, paired with the corresponding gold-standard analysis.
This data augmentation mitigates the small size of our manually disambiguated corpus (1,200 sentences).

The encoder processes each modified sentence independently, producing a different set of hidden states depending on the placement of the marker tokens.

This approach requires re-encoding the input sentence for each word to disambiguate.

Benefit: This increases the amount of training data, which is valuable for a low-resource language like Yakut, potentially helping the model generalize better.
Downside: It increases preprocessing complexity and memory requirements, as you’re storing multiple versions of each sentence.


For each sentence in the manually disambiguated Yakut corpus, you’d generate multiple training examples—one for each word in the sentence.
This increases the amount of training data, which is valuable for a low-resource language like Yakut, potentially helping the model generalize better.


To tokenize both the input and target sequences we use SentencePiece algorithm.
For the encoder, we train SentencePiece on a collection of sentences ...
To approximate Yakut morphemes, the vocabulary size is set to ...
The tokens \<TARGET> \</TARGET>

In the case of the decoder the algorithm is trained on the lexical roots from the morphological transducer.
The vocabulary size is set to ...
The affixes are hardcoded.


The model was trained with the following hyperparameters:

- Embedding Size: 128 for subword and marker token embeddings, learned during training.
- Hidden State Size: 128 for encoder and decoder LSTMs, balancing expressiveness and dataset size.
- Optimizer: Adam with a learning rate of 0.001.
- Dropout: 0.3 to prevent overfitting.
- Batch Size: 32, optimized for GPU training.

The training corpus consists of 1,200 manually disambiguated Yakut sentences, tokenized into subword units using a SentencePiece model trained on
the corpus.
Each sentence was preprocessed to generate modified sequences with <TARGET> and </TARGET> tokens enclosing each word’s subword tokens, paired with the gold-standard candidate analysis from the transducer.
Training was conducted for 15 epochs, with early stopping based on validation set performance.








## 5. Types of Morphological Ambiguity

Morphological ambiguity stems from diverse sources.
Hakkani-Tur [SOURCE] distinguishes for Turkish between part-of-speech ambiguity, segmentation ambiguity, morphographemic processes, and homograph morphemes.
All these types are also observed in the Yakut language, each posing different degrees of difficulty to solve.

For example, part-of-speech ambiguity can have its origin in the lexicon, when a lexical root is associated with more than one primary stem.
For instance, the root *санаа* can both be interpreted as a verbal stem (the second person imperative form of “to think”) or as a nominal stem (the singular nominative case of “thought”).
Affixation often helps to solve this type of lexical ambiguity, since stem types introduce restrictions on the affixes that can be attached to them.

Furthermore, lexicalized derivations, where derived forms develop independent and often idiomatic meanings, introduce additional ambiguities... [TUR].

Some monosyllabic adjectives coincide with verbs both in form and in content.
There are bases that are identical in sound composition, but completely different in meaning.
They are grammatical homonyms [§255, 161].

Segmentation ambiguity is very pervasive, since word forms often can be segmented in multiple different ways. This phenomenon is widespread in Yakut for disyllabic verbal roots that have a “fossilized” productive affix in the last syllable.
Read §500 about dead roots in reflexive verbs.

For example, the form *кэбис* can be analyzed as a lexical root with the sense of “to throw, toss, pitch, cast” or as *кэп* “push away, shove someone/something away“ + the reciprocal affix *-ис*.

Segmentation ambiguity also occurs with the combination of some affixes.
For example, it is observed systematically among the possessive cases in relation to the plural.
For instance, the form доруобуйаларыгар can be segmented both as:

доруобуйа+N+POSS.3PL+DAT
доруобуйа+N+PL+POSS.3SG+DAT

The root *доруобуйа* “health” can be interpreted as singular or plural.


In other cases, ambiguity can be introduced by morphographemic processes.
For example, the verbal roots *көп* “to rise, to float to the surface” and *көт* “to fly” have the same past participle form *көппүт*.
By regressive assimilation, an affix starting with a labial consonant like the past participle *-быт*, forces the final dental voiceless stop *(т)* in the stem көт to changes to a bilabial voiced stop *(п)*.
Because *көт* and *көп* are verbs, *көппүт* can only be disambiguated semantically.

Morphographemic processes render different interpretations of a surface form.
For example, *барыах* is analyzed as a verbal root *бар* “to go, leave, depart” with the future participle *-ыах*:

бар+V+PART_FUT

However, since affixes starting with vowels or diphthongs cause the deletion of long consonants in stem endings [SOURCE], the lexical root can also be interpreted as the verbal root *бараа* “to spend”:

бараа+V+PART_FUT

Yakut also has homograph morphemes.
For example, the predicate and possessive affixes in Yakut have the same form for the first and second person of the plural (*-быт*, *-гыт* and their allomorphs) [Ubryatova, 311].
Another example is observed for *-нан* and its allomorphs, which represents the instrument both in its plain and in its possessive variants.
When homograph affixes have similar grammatical function, syntax is not sufficient to disambiguate them.

Considering these various sources of ambiguity, the encoder-decoder model's successful processing of Yakut morphology hinges upon its ability to discern and resolve these key types – stemming from lexical variations, structural segmentation, phonological interactions, and formal overlaps – solely through the statistical regularities and contextual dependencies learned from the provided training data.


## 6. Experiment


This is particularly effective for Yakut’s local syntactic dependencies, such as agreement in case or number, where surrounding syllables provide critical cues.




## 7. Conclusions and Further Work


- The encoder captures contextual information from the sentence.
- The decoder ranks or generates the most likely morphological analysis.
- The attention mechanism focuses on relevant words in the sentence to resolve ambiguities.

Disambiguate particles and modals from nominal and verbal procedence.

1) Data augmentation
2) Syllable subword tokenization

Subword tokenization. This decision aims to achieve better interpretability and generalization.
Compared to subword units, which can vary greatly in length, syllables are more homogeneous segments.
This more consistent length facilitates the alignment of linguistic units with the attention weights, leading to more interpretable patterns.


As more morphological annotated data becomes available.


Framing morphological disambiguation as a seq2seq task with syllable-based
tokenization effectively captures Yakut’s morphological patterns, leveraging its
phonological structure to reduce sparsity. The <TARGET> and </TARGET> tokens guide the attention mechanism, improving focus on the target syllables
and their context. Preliminary experiments show a 4.5% improvement in sequence accuracy over a baseline without marker tokens, particularly for affixes
influenced by local context. Future work will explore transformer-based architectures and data augmentation techniques to further enhance performance in
this low-resource setting.

The proposed model follows an encoder-decoder framework with a Bahdanau attention mechanism, tailored to the morphological disambiguation task for Yakut’s agglutinative morphology.
The architecture is designed to leverage limited annotated data while effectively capturing contextual dependencies critical for disambiguating affixes and lexical roots.

The downside of this approach is the increases inference time.
As the encoder-decoder needs to process the sentence multiple times (once per word). For long sentences, this could be computationally expensive.

Re-encoding the sentence for each target word increases training and inference time, especially for longer sentences. If your Yakut sentences are long (e.g., >20 words), this could be a bottleneck.

A workaround to minimize this during inference time is to batch the input sentences for the words that require disambiguation.


Batch Candidates
When you batch the candidates, you are essentially passing multiple sequences through the encoder-decoder architecture in parallel, rather than one at a time. This approach takes advantage of parallelization (either on the CPU or GPU), reducing the overall time it takes to evaluate all candidates.

Conditional scoring for structured prediction


# References

[1] Hakkani-Tür D., Oflazer K., Tür G. (2002). Statistical Morphological Disambiguation for Agglutinative Languages, 2002.

[2] Cortegoso Vissio N., Khokhlova M.A. (2024). Three-Stage Morphological Pipeline for Word Form Analysis in Yakut. Conference: 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE), pp. 1860-1864.

[3] Cortegoso Vissio N.,

[4] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.
[5] Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. ICLR 2015.
[6] Hulden, M., & Francom, J. (2012). Boosting statistical tagger accuracy with simple rule-based grammars. LREC 2012.
[7] Silfverberg, M., Ruokolainen, T., Lindén, K., & Kurimo, M. (2016). FinnPos: An open-source morphological tagging and lemmatization toolkit for Finnish. Language Resources and Evaluation, 50(4), 863-878.
[8] Cotterell, R., Müller, T., Fraser, A., & Schütze, H. (2015). Labeled morphological segmentation with semi-Markov models. CoNLL 2015.
[9] Zalmout, N., & Habash, N. (2019). Adversarial multitask learning for joint multi-feature and multi-dialect morphological modeling. ACL 2019.
[10] Sheyanova, M., & Tyers, F. M. (2017). Annotation schemes in North Slavic corpora. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing.


## JUNK


Linguistic representation) tends to be structured, hierarchical, and highly dependent on linguistic theory, not just data frequency.



## 2. Previous Work



Sequence-to-sequence (seq2seq) learning was introduced by Sutskever et al. [[3]](#references), proposing an encoder-decoder architecture for mapping variable-length input sequences to variable-length output sequences.
Their model, based on recurrent neural networks, laid the foundation for many subsequent developments in machine translation and other structured prediction tasks.
The introduction of attention mechanisms by Bahdanau et al. [[4]](#references) further enhanced the seq2seq framework by allowing the decoder to selectively focus on relevant parts of the input sequence during generation, significantly improving performance on long and complex sequences.

Traditionally, encoder-decoder architectures have been used as generative models, producing outputs autoregressively based on previously generated tokens and the internal encoder representation.
However, several works have explored ways to adapt these models for discriminative tasks. Ranzato et al. [[5]](#references) proposed sequence-level training objectives for recurrent networks, showing that training models to rank entire candidate sequences can lead to better performance than token-level prediction. Similarly, Shen et al. [[6]](#references) introduced discriminative training strategies that enable models to directly optimize over candidate rankings rather than generation.
In information retrieval, MacAvaney et al. [[7]](#references) demonstrated that neural models with contextualized embeddings could be effectively trained to score and rank candidate outputs. Relatedly, Bahdanau et al. [[8]](#references) showed that attention-based models could be adapted for speech recognition tasks by discriminatively selecting among competing hypotheses during decoding.

While most work has focused on training models to generate or rank sequences in high-resource settings, relatively few efforts have investigated adapting encoder-decoder models for discriminative scoring in low-resource contexts, where only minimal annotated data is available.
Our work builds on these ideas by proposing a encoder-decoder architecture that scores candidate morphological analyses of a given word form, rather than generating them, allowing effective morphological disambiguation for Yakut despite the limited training data.



[3] Sutskever, I., Vinyals, O., & Le, Q.V. (2014). Sequence to Sequence Learning with Neural Networks. ArXiv.
[4] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate.
[5] Ranzato M.,  Chopra S., Auli M., Zaremba W. (2016). Sequence Level Training with Recurrent Neural Networks. International Conference on Learning Representations (ICLR).
[6] Shen Y., He X., Gao J., Deng L., Mesnil G. (2015). Training Neural Networks with Noisy Labels via Bootstrapping. International Conference on Learning Representations (ICLR) Workshop.
[7] MacAvaney S., Yates A., Cohan A., Goharian N. (2019). CEDR: Contextualized Embeddings for Document Ranking. Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval.
[8] Bahdanau D., Chorowski J., Serdyuk D., Brakel P., Bengio Y. (2016). End-to-End Attention-Based Large Vocabulary Speech Recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).




- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.




Morphological disambiguation is typically harder for agglutinative languages, such as Yakut for the following reasons:

- High Morpheme Density: A single word can encode multiple grammatical features, requiring disambiguation of overlapping or nested morpheme boundaries.

- Ambiguity in Segmentation: Morphemes may have homophonous forms (e.g., a suffix might mark a different morphological feature depending on context).

- Sparse Data: Rare morpheme combinations may not appear in training data, complicating statistical disambiguation.

The last point is particulary challenging for applying machine learning models (e.g., neural networks, CRFs) to the disambiguation task, as they rely on frequency statistics.
The training corpus may not contain enough examples of certain word forms or morpheme combinations to reliably estimate their probabilities.



The bootleneck is in the decoder part.




to select correct output among candidates in the context of the sequence.


- Annotated Data Scarcity
Morphologically annotated corpora are rare and often language-specific.
High-quality data is usually curated manually by linguists.







A morphological transducer generates an analysis representation from a word's surface form, capturing its grammatical structure and lexical properties.


Morphological disambiguation is the task of selecting the correct sequence of morphological parses corresponding to a sequence of words, from the set of possible parses for those word forms [1].

This disambiguation is particularly challenging for agglutinative languages like Yakut (Sakha), where a single root can generate numerous distinct word forms through sequential affixation of grammatical morphemes.



PREVIOUS WORK

This hybrid approach of combining rule-based transducers with statistical or neural ranking has been explored in various contexts. Hulden and Francom [6] demonstrated the effectiveness of combining finite-state transducers with statistical methods for morphological disambiguation in Spanish. Similarly, Silfverberg et al. [7] employed neural networks to rerank the outputs of a morphological generator, showing significant improvements over pure rule-based systems.

For morphologically rich languages, Cotterell et al. [8] explored neural morphological disambiguation as a structured prediction task, while Zalmout and Habash [9] developed a neural approach specifically for Arabic, using an LSTM-based architecture to select among analyses produced by an existing morphological analyzer.

Most relevantly to our work, Sheyanova and Tyers [10] applied neural reranking to morphological disambiguation for Turkic languages, demonstrating that even with limited annotated data, neural models can effectively leverage the outputs of rule-based transducers. However, their approach did not fully exploit the potential of attention mechanisms in capturing the relationship between word context and morphological features.

Our work builds upon these foundations by applying an encoder-decoder architecture with Bahdanau attention specifically to the task of ranking morphological analyses for Yakut. Unlike fully generative approaches, we use the decoder not to generate analyses autoregressively, but to score candidate analyses produced by a rule-based transducer within the context of the sentence.

